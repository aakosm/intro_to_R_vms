---
title: "Transforming and wrangling data 1/2"
subtitle: "Session Three"
author: "Akos Mate"
date: '2020 July'
output:
    html_document:
        code_folding: "show"
        number_sections: TRUE
        toc: true
        toc_depth: 4
        toc_float: true
        theme: flatly
        highlight: tango
        css: ../rmd_style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      collapse = TRUE,
                      comment = "#>",
                      message = FALSE
)
```


> Main packages used: `tidyr`, `dplyr`, `base R`     
> Main functions covered: `tidyr::pivot_longer`, `tidyr::pivot_wider`, `dplyr::group_by`, `dplyr::summarise`, `dplyr::bind_rows`, `dplyr::bind_cols` ,`runif`, `[`, `[[`  


> **Supplementary resources:**
>
> - [Data Wrangling cheat sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf)

# Tidyverse overview

From this session we will mostly start using packages from the `tidyverse` ecosystem. These include:

* `readr` for reading text data (.csv and .tsv)
* `tidyr` for reshaping your data
* `dplyr` for wrangling data (filtering your data, subsetting, transforming and recoding variables, etc.)
* `purrr` for functional programming
* `ggplot2` for data visualization
* `R markdown` for creating reports straight from R (.pdf, .html or .doc)

```{r, out.width = "550px", echo=FALSE}
knitr::include_graphics("figures/tidyverse1.png")
```

The "base R vs. tidyverse" debate has been going on in the R community for a while now, but it provides a quick and consistent way to start data analysis, strong community support and robust adoption rate in both inside and outside of academia. If you want some background thoughts on this, you can read a blogpost of David Robinson, data scientist at Heap on why teach tidyverse first: [http://varianceexplained.org/r/teach-tidyverse/](http://varianceexplained.org/r/teach-tidyverse/).

While the focus is on the tidyverse approach, the optional sections include alternative solutions, termed base R.


## The pipe

The pipe is this operator: ` %>% ` You can access with the shortcut of **`Ctrl+Shift+M`** (or type it out every time, but there are better things in life than that).  

What it does, is it passes object on left hand side as first argument of function on righthand side. 

As an example:
```{r, eval=FALSE}
head(mydata)

# OR

mydata %>% head()
```

Piping together various steps of our data manipulation process greatly increases code readability and our quality of life. In a moment we will see how piping can be super useful.


# Tidy data

Let's load our packages.

```{r}
library(dplyr) # for data manipulation
library(tidyr) # for reshaping data
library(readr) # loading data
```


We should use "tidy data" whenever possible. This means that each row is an observation and each column is a variable. This is best understood with this figure from the [R for Data Science](http://r4ds.had.co.nz/tidy-data.html) book:


```{r, out.width = "650px", echo=FALSE}
knitr::include_graphics("figures/tidy-data.png")
```

This format is often referred as the long data format. An example of the wide data format is where columns are values, not variables. The tidy version transforms this to create a year and inflation variables. To illustrate it with a made up dataset:

```{r echo=FALSE}
set.seed(2018)

wide_df <- data.frame(
    country = c("Hungary", "Belgium", "Mongolia"),
    `2006` = round(runif(3, min = 1, max = 3),1),
    `2007` = round(runif(3, min = 1, max = 4),1),
    `2008` = round(runif(3, min = 1, max = 5),1),
    `2009` = round(runif(3, min = 1, max = 2),1),
    check.names=FALSE
)

```



```{r echo=FALSE}
long_df <- wide_df %>% 
gather(key = "year", value = inflation, "2006":"2009")
```

The wide data format

```{r echo=FALSE, collapse=FALSE}
# wide data format
wide_df

```

And the transformed long data format.
```{r echo=FALSE, collapse=FALSE}
# long data format
long_df
```

Illustrating of what is happening from the [R for Data Science](http://r4ds.had.co.nz/tidy-data.html):
```{r, out.width = "650px", echo=FALSE}
knitr::include_graphics("figures/tidy_gather.png")
```

## Transforming datasets

We will use the `tidyr` package to shape our data into the desired format. A key concept is the key-value pairs. For the wide dataset above, the values are the inflation numbers and the keys are the column headers (years). So the `1.2` (2007 inflation in Hungary) and `2007` is such a key-value pair.

We will use this concept to our transformations. The main function we'll use from `tidyr` are the `pivot_longer()` to create tidy data from wide data format. 

Let's load our dummy dataset.

```{r}
wide_df <- read_csv("data/wide_df.csv")


wide_df
```


```{r eval=FALSE, echo=FALSE}

# we create a dummy dataset. The `runif()` function samples from a uniform distribution within the specified range.
wide_df <- data.frame(
    country = c("Hungary", "Belgium", "Mongolia"),
    `2006` = round(runif(3, min = 1, max = 3),1), # R does not allow numbers to be colnames, so use ` `
    `2007` = round(runif(3, min = 1, max = 4),1),
    `2008` = round(runif(3, min = 1, max = 5),1),
    `2009` = round(runif(3, min = 1, max = 2),1),
    check.names=FALSE # we tell R that it shouldn't worry our blatant use of numbers as colnames
)

wide_df

write.csv(wide_df, file = "data/wide_df.csv", row.names = FALSE)
```

Let's use the `tidyr::pivot_longer()` function and let's meet the ` %>% ` pipe operator (shortcut: **`Ctrl + Shift + M`**) again! Here, we specify the range of columns (`cols`) that contain our key-value pairs, and also the `names_to` (the year in this case) and the `values_to` (inflation).

```{r}
long_df <- wide_df %>% 
    tidyr::pivot_longer(cols = "2006":"2009", names_to = "year", values_to = "inflation")


long_df
```

Perfect! What if we want to go back to our wide format? Let's use the `tidyr::pivot_wider` function!

```{r}
long_df %>% 
    tidyr::pivot_wider(names_from = "year", values_from = "inflation")
    
```


## Appending data

While working with data, you rarely have the luxury to have all the data you need in one place. Let's recreate our previous data frame.

```{r}
country <- c("Thailand", "Norway", "Colombia", "Canada", "Slovenia", "France")
pop <- c(68.7, 5.2, 47.8, 35.8, 2, 63.6)

df1 <- data.frame(country, pop)
```

It just so happens that we now have data on their inflation and GDP growth. Let's simulate this data with the `runif` function. We want to extend our existing dataframe by adding the two new variable to it.

```{r}
v1 <- runif(n = 6, min = 1, max = 3) # we use the runif() function to sample from a uniform distribution
v2 <- runif(n = 6, min = -3, max = 3)

```

We have multiple options. First, assuming the order of the values in our new valuables matches the original one, we can simply add the two column. Just be mindful that here we match by *position*. For this we use the `bind_cols` from the `dplyr` package.

```{r}
df2 <- df1 %>% 
    dplyr::bind_cols(inflation = v1, gdp_pct = v2)

df2
```

If we have more observations (rows) we can slap them onto our dataframe as well. For this, we have a smaller dataframe, which contains the same variables, but with two different countries.

```{r}
df3 <- data.frame(
    country = c("Iceland", "Suriname"),
    pop = c(1.2, 0.6),
    inflation = c(0.1, 8.9),
    gdp_pct = c(0.5, 4.2))

df3

```

We should use the `bind_rows` function from `dplyr`.

```{r}
dplyr::bind_rows(df2, df3)
```


The corresponding `base R` commands are `cbind()` and `rbind()`.

# Quick detour: `base R` indexing and subsetting
(This section was adopted from [Martin Mölder](https://martinmolder.com/)'s 2017 course)

We have covered some parts of it already, but (if time permits) this section will give a short rundown of how to split datasets with `base R` indexing. While I think the `dplyr` approach that we will be using after this is more straightforward it is useful to at least know that there are multiple ways to access/subset/split datasets. Many online help that you'll see will either use the `dplyr` or the `base` approach and it is helpful if you can reverse engineer it.

To access a row or column in the data frame, remember that indexing in R works as `[row, column]`.

```{r}
# let's see the third row of our dataset
df2[3,]

# of course you can specify a vector of rows as well
df2[c(1:3),]

# or give individual columns if they don't happen to follow each other
df2[c(1,3),]

```


For splitting databases based on some conditions, let's remember our logical operators from before. Let's select all of the countries which are above 30 million in population. First, let's check how many values meet this criteria.

```{r}
df2$pop > 30 
```

This will give as a vector of logical values that has the same length as we have rows in the data set. And we can use that to select rows – the rows with `TRUE` are selected and the rows with `FALSE` are not. We just have to put the vector of logical values where the row index of the data object is.

```{r}
df2[df2$pop > 30, ]
```

We can apply this conditional logic to selecting columns as well. We use the `|` (OR) operator.

```{r collapse=FALSE}
names(df2) == "pop"

names(df2) == "gdp_pct"

vars <- names(df2) == "pop" | names(df2) == "gdp_pct"

vars

df2[,vars]

# or just use the names of your variables directly
df2[,c("pop", "gdp_pct")]
```

